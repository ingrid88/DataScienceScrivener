{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf340
\cocoascreenfonts1{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red53\green53\blue53;\red220\green161\blue13;}
\deftab560
\pard\pardeftab560\slleading20\partightenfactor0

\f0\fs24 \cf2 W7D5\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
\
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 Eigen values\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 there are many ways to do feature selection and reduction of features. \
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 take features and create new features and feature selection on the new features\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 you can change frame of reference and combine features \
represent data in a lower dimensional space, and still be able to recreate our original data.   retain most info in lower dimensional space. Problem; loose interpretability\
\
\
D2 \
\
uncorrelated features are all normal or perpendicular to one another. so that\'92s great\'85 we optimize our feature space to only include the essentials . they orthogonal to one another. \
\
# principle e components as # features but you can specify\
\
features selection removes\
feature extraction keeps linear combination of both\
\
Even though you have orthogonal features, although you\'92ve now successfully represented your feature space in the fewest dimensions, it doesn\'92t mean they\'92ll be the most useful for your model. Right?\
\
SCREE plots\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
Kernel PCA (project into higher space do pCa and then bring it back down.)\
\
\
PCA analysis, you get same number of components as features but some have coefficients that explain for more of the variance. They are ordered (something to do with eigenvalues) so you can choose the top set of them. These new pca components are all perpendicular, so there is NO correlation. \
\
kernels like RBF {\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Radial_basis_function"}}{\fldrslt \cf3 https://en.wikipedia.org/wiki/Radial_basis_function}}\
\
swiss roll. you can simplify by changing frame of reference. \
\
stochastic neighborhood embedding\
\
isomap - local structure, who\'92s close to each other, \
tcne \
local linear imbedding\
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 multiple dimmentional scalling - compare distances between \'85relative distances are the same in the lower space. \
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
\
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 eigenvalue is the scale\'85.\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 latent semantic analysis or indexing (words with similar semantic meaning) how can we do this: PCA ????    }