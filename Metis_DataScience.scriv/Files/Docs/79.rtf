{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf340
\cocoascreenfonts1{\fonttbl\f0\fnil\fcharset0 Cochin;}
{\colortbl;\red255\green255\blue255;}
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural

\f0\fs28 \cf0 Themes within text: \
Distribution of possible words (probability distribution over all possible words)\
\
\
Lets use an algorithm spcifically developed to find topics\
LDA = Andrew ing, david bli, michael jordan\
Latent Dirilicht allocation\
\
Gibbs Sampling\
\
\
In writing, we choose a topic, and then write words from the distrubution of words in that topic\
\
Generative model (can we understand how this data itself was generated)\
\
Alpha parameter = corupus\
Theta = each doc\
Zeta = each word (topic)\
W = each specific word\
\
\
Requires dimensionality reduction (clustering, classifying)\
\
}